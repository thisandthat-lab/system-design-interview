# 9장. S3와 유사한 객체 저장소

## 저장소 시스템 101

### 블록 저장소

- HDD, SSD같은 서버에 물리적으로 연결되는 형태의 드라이브는 블록 저장소의 가장 흔한 형태이다.
    - 물리 연결로 국한되지 않고, 고속 네트워크나 FC, iSCSI 프로토콜을 통해 연결될 수도 있다.
- raw block을 서버에 볼륨 형태로 제공하는, 가장 유연하고 융통성이 높은 저장소이다.

### 파일 저장소

- 블록 저장소 위에 구현되며, 파일과 디렉토리를 손쉽게 다루는 데 필요한 높은 수준의 추상화를 제공한다.
- 데이터는 계층적으로 구성되는 디렉터리 안에 보관된다.
- 폴더나 파일을 같은 조직 구성원에 공유하는 솔루션으로 사용하기 좋다.

### 객체 저장소

- 실시간으로 갱신할 필요가 없는 cold 데이터 보관에 초점을 둔다.
- 데이터 아카이브나 백업에 주로 쓰인다.
- 데이터 영속성을 높이고 대규모 애플리케이션을 지원하며 비용을 낮추기 위해 의도적으로 성능을 희생한다.
- 모든 데이터를 수평적 구조 내에 객체로 보관하며, 계층적 디렉토리 구조는 제공하지 않는다.

### 객체 저장소 용어 정리

- **버킷**: 객체를 보관하는 논리적 컨테이너
- **객체**: 버킷에 저장되는 개별 데이터
- **버전**: 한 객체의 여러 버전을 같은 버킷 안에 둘 수 있도록 하는 기능
- **URI**: 객체에 접근할 수 있는 고유한 식별자
- **SLA**: 서비스 수준 협약(S3는 연간 99.9%의 가용성 제공)

# 1단계. 문제 이해 및 설계 범위 확정

### 기능 요구사항

- 버킷 생성
- 객체 업로드 및 다운로드
- 객체 버저닝
- 버킷 내 객체 목록 출력(aws s3 ls 명령어와 유사하도록 구현)

### 비기능 요구사항

- 매년 100PB 데이터 추가
- 식스 나인(99.9999%) 수준의 데이터 내구성 제공
- 포 나인(99.99%) 수준의 서비스 가용성 제공
- 저장소 효율성(높은 수준의 안정성과 성능 보장, 저장소 비용은 최대한 낮추어야 함)

### 개략적 규모 추정

- 디스크 용량이나 초당 디스크 IO(IOPS)가 병목이 될 가능성이 높다.
- **디스크 용량**
    - 객체 중 20%는 1MB 이하, 60%는 1~64MB, 20%는 64MB 이상이라고 가정 (중앙값을 사용하여 계산)
        - 100PB = 10^11MB
        - 10^11 * 0.4 / 0.2*0.5MB + 0.6*32MB + 0.2*200MB = 6억 8천만 개 객체
            - 모든 객체의 메타데이터 크기가 1KB라고 가정하면, `0.68TB` 공간 필요
- **IOPS**
    - 인터페이스를 탑재하고 7200rpm을 지원하는 하드디스크 하나가 초당 100~150회의 탐색을 지원한다고 가정
        - `100 ~ 150 IOPS`

# 2단계. 개략적 설계안 제시 및 동의 구하기

## 객체 저장소의 속성

- **객체 불변성**
    - 객체 저장소에 보관되는 객체들은 변경이 불가능하다.
    - 변경하기 위해서는 삭제한 다음 새 버전 객체로 완전히 대체해야 한다. (값을 점진적으로 변경할 수는 없음)
- **키-값 저장소**
    - 해당 객체의 URI를 사용하여 데이터를 가져올 수 있다.
    - URI가 키에 해당, 데이터가 값에 해당하므로 키-값 저장소로 볼 수 있다.
- **저장은 1회, 읽기는 여러 번**
    - 객체 저장소에 대한 요청 가운데 95%가량이 읽기 요청이다.
- **소형 및 대형 객체 동시 지원**
    - 다양한 크기의 객체를 문제 없이 저장할 수 있다.

## 객체 저장소와 UNIX 파일 저장소 비교

- **UNIX 파일 시스템**
    - 아이노드
        - 파일 이름, 소유자 UID, 그룹 UID
    - 객체 접근 순서
        - 파일 블록 포인터
        - 로컬 디스크 접근
        - 하드 디스크

- **객체 저장소 시스템**
    - 메타데이터 저장소
        - 객체 이름 → 객체 ID
    - 객체 접근 순서
        - 객체 ID
        - 네트워크 요청
        - 데이터 저장소

## 개략적 설계안

- **LB**
    - RESTful API 요청을 API 서버들에 분산한다.
- **API 서비스**
    - IAM서비스, 메타데이터 서비스, 저장소 서비스에 대한 호출을 조율한다.
    - stateless 서비스이므로 수평적 규모 확장이 가능하다.
- **IAM 서비스**
    - 인증, 권한 부여, 접근 제어 등을 중앙에서 맡아 처리한다.
- **데이터 저장소**
    - 실제 데이터를 보관하고 필요할 때마다 읽어가는 장소이다.
    - 모든 데이터 관련 연산은 UUID로 이루어진다.
- **메타데이터 저장소**
    - 객체 메타데이터가 보관된다.

## 객체 업로드

- 클라이언트는 버킷 생성을 위한 HTTP PUT 요청을 API 서비스로 보낸다.
- API 서비스에서 하는 일
    - IAM을 호출하여 해당 사용자에게 Write 권한이 있는지 확인한다. (메타데이터)
    - 메타데이터 DB에 버킷 정보를 등록하고 클라이언트에게 알린다.
        
        → 클라이언트가 객체 생성을 위한 HTTP PUT 요청을 다시 API 서비스로 보낸다.
        
    - IAM을 호출하여 해당 사용자에게 Write 권한이 있는지 확인한다. (객체)
    - 문제가 없다면 객체를 데이터 저장소로 보내고 UUID를 받아온다.
    - 메타데이터 DB에 데이터를 생성한다.

## 객체 다운로드

- 버킷은 디렉토리같은 계층 구조를 지원하지 않는다.
    - 버킷 이름과 객체 이름을 연결하면 폴더 구조같은 논리적 계층을 만들 수 있다. ex) bucket-to-share/script.txt
- 요청 흐름
    - 클라이언트가 GET {객체명} 요청을 LB로 보내고, LB는 API 서버에 요청을 전달한다.
    - API 서비스에서 하는 일
        - IAM에 질의하여 해당 버킷에 READ 권한이 있는 지 확인한다.
        - 권한이 있으면 UUID를 메타데이터 DB에서 가져와 데이터 저장소에서 객체를 가져온 후 클라이언트에 반환한다.

# 3단계. 상세 설계

## 데이터 저장소

- 객체 업로드 - payload: 파일 내용 / response: ObjectId
- 객체 다운로드 - payload: ObjectId / response: 파일 내용

## 데이터 저장소의 개략적 설계

![IMG_7700](https://github.com/thisandthat-lab/system-design-interview/assets/132281360/1bf4cd84-d963-417d-bf57-ea30f4721832)

### ⚙️ 데이터 라우팅 서비스

- 데이터 노드에 접근하기 위한 RESTful 또는 gRPC 서비스 제공
- 배치 서비스를 호출하여 데이터를 저장할 최적의 데이터 노드 판단
- 데이터 노드에 데이터 CRUD

### ⚙️ 배치 서비스

- 어느 노드에 데이터를 저장할 지 결정
- 내부적으로 가상 클러스터 맵을 가지고 있음
    - 여기 보관되는 데이터 노드의 위치 정보를 이용해 데이터 사본이 물리적으로 다른 위치에 놓이도록 함
- 모든 데이터 노드와 지속적으로 박동 메시지를 주고받으며 상태 모니터링
    - 15초동안 응답하지 않는 노드는 클러스터 맵에 down 상태로 기록
- 아주 중요한 서비스이므로 5~7개 노드를 갖는 배치 서비스 클러스터를 Paxos, Raft같은 합의 프로토콜로 구현할 것을 권장
    - 일부 노드에 장애가 발생해도 건강한 노드 수가 클러스터 크기의 절반 이상이면 서비스 지속 보장

### ⚙️ 데이터 노드

- 실제 객체 데이터가 보관되는 곳
- 여러 노드에 데이터를 복제함으로써 데이터의 안정과 내구성 보증 (**다중화 그룹**)
- 각 노드에는 배치 서비스에 주기적으로 **박동 메시지**를 보내는 서비스 데몬이 돌고 있음
    - 해당 데이터 노드에 부착된 디스크 드라이브(HDD/SDD)의 수
    - 각 드라이브에 저장된 데이터의 양
- 배치 서비스에서는 특정 노드에 대한 박동 메시지를 처음 받으면 아래와 같은 일을 수행한다.
    - 해당 데이터 노드에 고유한 식별자 부여
    - 응답: 고유 식별자, 가상 클러스터 지도, 데이터 사본 보관 위치

### 데이터 저장 흐름

- API서비스가 데이터 저장소로 데이터를 포워딩하면, **데이터 라우팅 서비스**에서 캐치한다.
    - 해당 객체에 UUID를 할당하고 배치서비스를 참조하여 주 노드를 선정한다.
        
        → 다중화 그룹의 추가/삭제에도 유지되어야 하므로 안정 해시를 사용한다.
        
    - 저장할 데이터를 UUID와 함께 주 노드에 전송한다.
- 주 노드는 데이터를 자기 노드에 저장하고, 부 데이터 노드에 다중화한다.
- 모든 부 데이터 노드에 성공적으로 다중화되면 데이터 라우팅 서비스에 응답을 보낸다.
    
    → 데이터 일관성과 지연시간 사이의 트레이드오프를 고려하자.
    
- 데이터 라우팅 서비스는 객체의 UUID를 API서비스에 반환한다.

## 데이터는 어떻게 저장되는가

- 각각의 객체를 개별 파일로 저장한다. → 작은 파일이 많아진다면?
    - 낭비되는 데이터 블록 수가 늘어난다.
    - 시스템의 아이노드(파일 위치 등의 정보 저장) 용량 한계를 초과한다.
- 작은 객체들을 큰 파일 하나로 모아서 저장한다. (WAL과 같이 객체를 저장할 때 이미 존재하는 파일에 추가하는 방식)
    - 용량 임계치에 도달한 파일은 읽기 전용 파일로 변경하고 새로운 파일을 만든다.
    - 읽기 전용으로 변경된 파일은 읽기 요청만 처리한다.
    - 객체는 파일 안에 일렬로 저장된다.
        - CPU 코어가 쓰기 연산을 병렬로 진행하더라도 객체 내용이 섞이지 않아야 한다.
        - 파일에 객체를 기록하기 위해서는 자기 순서를 기다려야 하므로 대역폭이 줄어든다는 문제가 있다.
        - 이를 해결하기 위해 서버에 오는 요청을 처리하는 코어별로 전담 읽기-쓰기 파일을 두어야한다.

## 객체 소재 확인

- 각각의 데이터 파일 안에 많은 작은 객체가 들어있다면, 데이터 노드는 어떻게 UUID로 객체 위치를 찾을 수 있을까?
    - 알아야 하는 정보
        - 객체가 보관된 데이터 파일
        - 데이터 파일 내 객체 오프셋
        - 객체 크기
- 위의 정보를 저장하는 방법
    - key-value 저장소
        - RocksDB는 SSTable에 기반한 방법으로, 쓰기 연산 성능은 좋지만 읽기 성능은 느리다.
    - RDB
        - 보통 B+ 트리 기반 저장 엔진을 이용하며, 읽기 성능은 좋지만 쓰기 성능은 느리다.
    
    → 데이터 접근 패턴이 읽기 연산이 압도적으로 많으므로 RDB가 더 나은 선택이다.
    
    → 데이터 노드마다 RDB를 설치한다. (SQLite)
    

## 개선된 데이터 저장 흐름

- 데이터 노드 서비스는 객체를 읽기-쓰기 파일(여러 개 중 하나) 마지막 부분에 추가한다.
- 해당 객체에 대한 새로운 레코드를 **object_mapping** 테이블에 추가한다.
    - **object_mapping(객체 위치 데이터베이스)**: obj_id, file_name, offset, obj_size

## 데이터 내구성

### 하드웨어 장애와 장애 도메인

- 하드 디스크 장애는 피할 수 없다.
- 데이터를 여러 대의 하드 드라이브에 복제하여 어떤 드라이브에서 발생한 장애가 전파되지 않도록 한다.
- 완전한 내구성 평가를 위해서는 여러 **장애 도메인**의 영향을 복합적으로 고려해야 한다.
    - **장애 도메인**: 중요한 서비스에 문제가 발생했을 때 부정적인 영향을 받는 물리적/논리적 구획
        - ex) 랙 - 스위치, 파워 서플라이 / AZ

### 소거 코드

- 데이터의 일부가 소실되었을 때 복구하기 위한 **패리티**라는 정보를 만들어 중복성을 확보한다.
    
    → 장애가 생기면 남은 데이터와 패리티를 조합하여 소실된 부분을 복구한다.
    
- 데이터를 다중화할 경우 하나의 객체를 읽을 때 여러 대의 건강한 노드를 참조해야 한다는 구조적 단점이 있다.
- 데이터 읽기 시 응답 지연은 높아지는 대신 내구성은 향상되고 저장소 비용은 낮아진다. (trade-off)

→ 응답 지연이 중요한 애플리케이션은 다중화가 좋고, 저장소 비용이 중요한 애플리케이션은 소거 코드가 좋다.

### 정확성 검증

- 디스크가 아니라 메모리의 데이터가 망가졌을 때 어떻게 대처할 지 알아보자.
    
    → 프로세스 경계에 데이터 검증을 위한 체크섬을 두어 해결한다.
    
    - **체크섬**: 데이터 에러를 발견하는 데 사용되는 작은 크기의 데이터 블록
    - 새로 계산한 체크섬이 원본 체크섬과 다르면 데이터가 망가졌다고 판단한다.
- 소거 코드와 체크섬 확인 매커니즘 동시에 활용하기
    - 객체 데이터와 체크섬을 가져온다.
    - 수신된 데이터의 체크섬을 계산한다.
        - 두 체크섬이 일치하면 데이터는 완전하다.
        - 체크섬이 다르면 데이터가 망가진 것이므로 다른 장애 도메인에서 데이터(패리티?)를 가져와 복구한다.
    - 원래 객체 복원이 완료되면 클라이언트에게 보낸다.

## 메타데이터 데이터 모델

- 지원해야 하는 질의
    - 객체 이름으로 객체 ID 찾기
    - 객체 이름에 기반하여 객체 삽입/삭제
    - 같은 접두어를 갖는 버킷 내의 모든 객체 목록 확인
- 이 조건을 만족하는 데이터베이스 스키마
    - **bucket** - bucket_name, bucket_id, owner_id, enable_versioning
    - **object** - bucket_name, object_name, object_version, object_id

### bucket 테이블의 규모 확장

- 한 사용자가 만들 수 있는 버킷 수에는 제한이 있으므로 테이블 크기는 작다.
- 읽기 부하가 걱정된다면 사본을 만들어 읽기 부하를 분산한다.

### object 테이블의 규모 확장

- 메타데이터를 디비 서버 한 대에 보관하기는 불가능하므로 **샤딩**을 통해 규모를 확장한다
    - **bucket_id** 기준
        - 버킷 안에 수십억 개 객체가 있는 핫스팟 샤드를 지원하지 못하므로 좋은 방안이 아니다.
    - **object_id** 기준
        - 부하를 균등하게 분산할 수 있지만, URI 기준 질의를 효율적으로 지원하지 못한다.
    - **bucket_name + object_name** 결합
        - URI 기준 질의도 지원되고, 데이터도 균등하게 분산된다.
        - ‘같은 접두어를 갖는 버킷 내의 모든 객체 목록 확인’ 질의는 애매하다.

### 버킷 내 객체 목록 확인

- 객체는 계층적 구조가 아닌 수평적 경로로 접근한다.
- 사용자가 버킷 내 객체들을 잘 정리할 수 있도록 S3는 ‘접두어’라는 개념을 지원한다.
    - 접두어를 통해 계층적 구조를 흉내낼 수 있다.
    - 접두어는 디렉토리가 아니다.
    - 상황에 맞게 질의 명령어를 사용하자.

### 단일 데이터베이스 서버

- 같은 접두어를 갖는 버킷 내 모든 객체를 출력하려면 `Like ‘abc/%’`와 같이 질의한다.

### 분산 데이터베이스

- 메타데이터 서버가 모든 샤드에 `Like ‘abc/%’` 질의를 돌리고, 메타데이터 서비스는 취합하여 전달한다.
    - 파티셔닝 기능을 구현하기 복잡하다.
        - 서버는 모든 샤드의 오프셋을 추적하여 커서에 결부시킬 수 있어야 한다.
        - 이 문제를 해결하기 위해 별도 테이블에 목록 데이터를 비정규화하는 방법이 있다.

## 객체 버전

- 버킷 안에 한 객체의 여러 버전을 둘 수 있도록 한다.
- 객체가 수정되면 해당 문서의 이전 메타데이터는 새 메타데이터로 대체되고, 이전 문서는 삭제로 표시된다.
    - 버전 기능이 활성화되었을 경우 object_version이라는 열이 사용된다.
    - 기존 레코드를 덮어쓰는 대신, 새로운 값인 레코드를 추가하고 새 객체의 UUID를 반환한다.
- 삭제 플래그가 있는 문서는 가비지 컬렉터가 회수해간다.

## 큰 파일의 업로드 성능 최적화

- **멀티파트** 업로드
    - 큰 객체는 작게 쪼갠 다음 독립적으로 업로드한다.
    - 모든 조각이 업로드되고 나면 객체 저장소는 그 조각을 모아 원본 객체를 복원한다.
- 객체 조립이 끝난 뒤 조각은 더 이상 쓸모가 없어진다.
    
    → 이런 조각을 삭제하여 저장 용량을 확보하는 가비지컬렉션 프로세스를 구현해야 한다.
    

## 가비지 컬렉션

- 쓰레기 데이터가 생기는 경우
    - **객체의 지연된 삭제**: 삭제 플래그는 남기되 실제로 지우지는 않음
    - **고아 데이터**: 반쯤 업로드 된 데이터, 취소된 멀티파트 업로드 데이터
    - **훼손된 데이터**: 체크섬 검사에 실패한 데이터
- 바로 지우지 않고 정리 매커니즘을 주기적으로 실행하여 지운다.
- 가비지 컬렉션의 정리 매커니즘
    - 삭제할 객체를 복사해온다.
    - 모든 객체를 복사한 다음 **object_mapping** 테이블을 갱신한다. (새 위치를 가리키도록)

# 4단계. 마무리

- LGTM 🚀
