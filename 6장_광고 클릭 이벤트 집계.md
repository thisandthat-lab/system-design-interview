# 6장. 광고 클릭 이벤트 집계

- 온라인 광고의 핵심적 혜택: 실시간 데이터를 통해 광고 효과를 정량적으로 측정할 수 있다는 점
- 디지털 광고의 핵심 프로세스: **RTB(Real-Time Bidding)**, 즉 실시간 경매 절차를 통해 광고가 나갈 지면을 거래한다.
    - 속도가 매우 중요하다. (보통 1초 이내에 모든 프로세스가 마무리 되어야 함)
    - 데이터의 정확성도 중요하다. (핵심지표 - CTR, CVR)

# 1단계. 문제 이해 및 설계 범위 확정

## 기능 요구사항

- 입력 데이터는 여러 서버에 분산된 로그 파일이고, 클릭 이벤트는 수집될 때마다 로그 파일의 끝에 추가된다.
- 다음의 3가지 질의를 지원해야 한다.
    - 특정 광고에 대한 지난 M분간의 클릭 이벤트 수
    - 지난 1분간 가장 많이 클릭 된 광고 100개
    - ip, user_id, country 등의 속성을 기준으로 상기 2개 질의 결과 필터링
- 엣지 케이스 고려
    - 예상보다 늦게 도착하는 이벤트에 대한 처리
    - 중복된 이벤트에 대한 처리
    - 시스템 다운에 대한 대응

## 비기능 요구사항

- 모든 처리는 수 분 내에 이루어져야 한다.
- RTB 지연 시간은 1초 미만이어야 하지미ㅏㄴ, 광고 클릭 이벤트 집계는 몇 분 정도의 지연을 허용한다.

## 개략적 추정

- DAU 10억명, 각 사용자는 하루 평균 1개 광고를 클릭한다고 가정
    
    → 하루에 10억건의 광고 클릭 이벤트 발생
    
    → 광고 클릭 QPS = 10^9 이벤트 / 하루 10^5초 = 10,000
    
    → 최대 광고 클릭 QPS는 평균 QPS의 5배, 즉 50,000
    
- 광고 클릭 이벤트 하나 당 0.1KB 저장 용량이 필요하다고 가정
    
    → 일일 저장소 요구량 = 0.1KB * 10억 = 100GB
    
    → 월간 저장 용량 요구량 = 약 3TB
    

# 2단계. 개략적 설계안 제시 및 동의 구하기

## 질의 API 설계

- 클라이언트: 대시보드를 이용하는 데이터 사이언티스트, 제품 관리자, 광고주 등

### [API 1] 지난 M분간 각 ad_id에 발생한 클릭 수 집계

- `GET` /v1/ads/{:ad_id}/aggregated_count
- payload
    - from(long) : 집계 시작 시간
    - to(long) : 집계 종료 시간
    - filter(long) : 필터링 전략 식별자
- response
    - ad_id(string) : 광고(ad) 식별자
    - count(long) : 집계된 클릭 횟수

### [API 2] 지난 M분간 가장 많은 클릭이 발생한 상위 N개 ad_id 목록

- `GET` /v1/ads/popular_ads
- payload
    - count(integer) : 상위 몇 개의 광고를 반환할 지 지정
    - window(integer) : 분 단위로 표현된 집계 윈도 크기
    - filter(long) : 필터링 전략 식별자
- response
    - ad_ids(array) : 광고 식별자 목록

## 데이터 모델

### 원시 데이터

- 로그에 기록되는 text 포맷의 데이터를 의미한다.
- 원시 데이터가 여러 어플리케이션 서버에 산재해 있다.

### 집계 결과 데이터

- 집계 결과를 ad_id, click_minute, filter_id, count로 분류하여 저장한다.

### 원시 데이터 저장 vs 집계 결과 데이터 저장 비교

- 원시 데이터만 저장했을 때
    - 원본 데이터를 손실 없이 보관 가능하고, 데이터 필터링과 재계산을 지원한다.
    - 데이터 용량이 너무 크고 질의 성능이 낮다.
- 집계 결과 데이터만 저장했을 때
    - 데이터의 용량을 절감하고, 빠른 질의 성능을 보인다.
    - 원본 데이터는 손실되고, 계산/유도된 데이터를 저장하는 데서 오는 결과만을 가지고 있다.

→ 원시 데이터, 집계 결과 데이터 둘 다 저장하는 것이 좋다.

- 문제 발생 시 원시 데이터를 디버깅과 재계산에 활용할 수 있다.
- 질의 성능을 높이기 위해 집계 결과 데이터가 있고, 백업의 용도로 원시 데이터가 존재하는 것이다.

## 올바른 데이터베이스의 선택

### 올바른 데이터베이스 선택을 위한 고려사항

- 데이터의 형태 - 관계형 데이터 or 문서 데이터 or BLOB 등등
- 작업 흐름 - 읽기 중심 or 쓰기 중심 or 둘 다
- 트랜잭션 지원 필요 여부
- OLAP 함수 (SUM, COUNT 등) 지원 필요 여부

### 원시 데이터

- 데이터 백업과 재계산 용도로만 이용되므로 읽기 연산 빈도는 낮으며, 쓰기 빈도는 높다.
    
    → 쓰기 및 시간 범위 질의에 최적화 된 **카산드라, InfluxDB**를 사용하는 것이 바람직하다.
    
    → 혹은 ORC, 파케이, AVRO같은 컬럼형 데이터 형식을 사용하여 S3에 데이터를 저장할 수도 있다.
    

### 집계 데이터

- 시계열 데이터이며, 읽기와 쓰기 연산 둘 다 많이 발생한다.
    
    → 원시 데이터와 같은 유형의 데이터베이스를 활용한다.
    

## 개략적 설계안

### 비동기 처리

- 동기식 시스템의 경우 특정 컴포넌트의 장애가 전체 시스템의 장애로 이어질 수 있다.
    
    → 카프카 같은 메시지 큐를 도입하여 생산자와 소비자와의 결합을 끊는다.
    
    → 전체 프로세스는 비동기 방식으로 동작하게 되고, 생산자와 소비자의 규모를 독립적으로 확장할 수 있다.
    
- 개략적 설계안
    - 로그 모니터 → **MQ 1** → 데이터 집계 서비스 → **MQ 2** → 집계 결과 DB
    - 로그 모니터 → **MQ 1** → 원시 데이터 DB
    
    ---
    
    - **MQ 1**: 광고 클릭 이벤트 데이터가 기록된다. (ad_id, click_timestamp, user_id, ip, country)
    - **MQ 2**: 분 단위로 집계된 광고 클릭 수, 분 단위로 집계한 가장 많이 클릭한 상위 N개 광고가 기록된다.
        
        → 정확하게 한 번(exactly once) 데이터를 처리하기 위해 kafka같은 시스템을 도입해야 한다.
        

### 집계 서비스

- 맵리듀스 프레임워크를 사용하면 좋다.
    - 맵리듀스 프레임워크에 좋은 모델은 **DAG 모델**이다.
        - 시스템을 맵/집계/리듀스 노드 등의 작은 컴퓨팅 단위로 세분화한다.
        - 각 노드는 한 가지 작업만 처리하며, 처리 결과를 다음 노드에 인계한다.
- **맵리듀스 프레임워크**
    - **맵 노드**
        - 데이터 출처에서 읽은 데이터를 필터링하고 변환하는 역할을 담당한다.
        - 맵 노드가 왜 필요할까?
            - 입력 데이터를 정리하거나 정규화해야 하는 경우 필요하다.
            - 데이터 생성 방식에 대한 제어권이 없다면 동일 ad_id를 갖는 이벤트가 서로 다른 파티션에 입력될 수 있다.
    - **집계 노드**
        - ad_id별 광고 클릭 이벤트 수를 매 분 메모리에서 집계한다.
        - 맵리듀스 패러다임에서 집계 노드는 리듀스 프로세스의 일부이다.
    - **리듀스 노드**
        - 모든 집계 노드가 산출한 결과를 최종 결과로 축약한다.
            - 집계 노드 각각은 자기 관점에서 가장 많은 클릭이 발생한 광고 3개를 추려 리듀스 노드로 보낸다.
            - 리듀스 노드에서는 그 결과를 모아 최종적으로 상위 M개의 광고만 남긴다.
- **DAG**
    - 맵리듀스 패러다임을 표현하기 위한 모델이다.
    - 빅데이터를 입력으로 받아 병렬 분산 컴퓨팅 자원을 활용하여 작은 크기의 데이터로 변환할 수 있도록 설계된 모델이다.
    - 이 모델의 중간 데이터는 메모리에 저장될 수 있다.
    - 노드 간 통신은 TCP로 처리될 수도 있고, 공유 메모리로도 처리할 수 있다.
- **스타 스키마**
    - 데이터 필터링을 지원하기 위해 필터링 기준을 사전에 정의한 다음 해당 기준에 따라 집계한다.
    - 데이터 웨어하우스에서 널리 쓰이는 기법으로, 필터링에 사용되는 필드는 차원(디멘션)이라고 부른다.
    - 이해하기 쉽고 구축하기 간단하다.
    - 기존 집계 서비스를 재사용하여 스타 스키마에 더 많은 차원을 생성할 수 있다.
    - 결과를 미리 계산해두는 방식이므로, 필터링 기준에 따라 데이터에 빠르게 접근할 수 있다.

# 3단계. 상세 설계

## 스트리밍 VS 일괄 처리

- 본 설계안은 스트림 처리와 일괄 처리 방식을 모두 사용한다. → **람다(lambda) 아키텍처**
    - **스트림 처리**: 데이터를 오는 대로 처리하고, 실시간으로 집계된 결과 생성
    - **일괄 처리**: 이력 데이터 백업
- 람다 아키텍처는 두 가지 처리 경로를 지원해야 하므로 관리 포인트가 두 벌이 된다는 단점이 있다.
    
    → **카파 아키텍처**는 일괄 처리와 스트리밍 처리 경로를 하나로 결합하여 문제를 해결한다.
    
    - 단일 스트림 처리 엔진을 사용하여 실시간 데이터 처리 및 데이터 재처리 문제를 모두 해결한다.

## 데이터 재계산

- 재계산 프로세스
    - 원시 데이터 저장소에서 데이터를 검색한다. (일괄 처리 프로세스)
    - 추출된 데이터는 전용 집계 서비스로 전송된다.
        - 데이터 처리 과정이 과거 데이터 재처리 프로세스와 간섭하는 일 방지
    - 집계 결과는 두 번째 MQ로 전송되어 집계 결과 DB에 반영된다.
- 재계산 프로세스는 데이터 집계 서비스를 재사용하기는 하지만, 처리 대상 데이터는 다른 곳에서 읽는다. (원시 데이터)

## 시간

- 집계를 하기 위해 필요한 타임스탬프
    - **이벤트 시각**: 광고 클릭이 발생한 시각
    - **처리 시각**: 집계 서버가 클릭 이벤트를 처리한 시스템 시각
- 이벤트 발생 시각을 집계에 사용하는 경우 이벤트 처리 문제를 잘 해결해야 한다.
    - 네트워크 지연이나 비동기적 처리 환경때문에 이벤트 발생 시각과 처리 시각의 격차가 커질 수 있다.
    - 처리 시각을 집계에 사용하는 경우 집계 결과가 부정확할 수 있다.
- **이벤트 발생 시각으로 집계 vs 처리 시각으로 집계**
    - 이벤트 발생 시각으로 집계 ✅
        - 집계 결과가 보다 정확하지만, 클라이언트가 생성한 타임스탬프에 의존한다는 단점이 있다.
        - 데이터의 정확도가 중요한 시스템에서는 이 방식을 채택한다. (우리 시스템도)
    - 처리 시각으로 집계
        - 클라이언트 타임스탬프보다 안정적이지만, 이벤트 도착 시간이 한참 뒤인 경우 부정확해진다는 단점이 있다.
- **워터마크** 기술을 활용해서 시스템에 늦게 도착한 이벤트를 올바르게 처리한다.
    - 윈도우마다 N초 가량의 워터마크를 붙인다.
    - 워터마크 구간이 길면 늦게 도착하는 이벤트를 포착할 수 있지만, 시스템의 이벤트 처리 시간이 늘어난다.
    - 워터마크가 짧으면 데이터 정확도는 떨어지지만, 시스템의 응답 지연은 낮아진다.
    - 시간이 한참 흐른 후에 도달하는 이벤트는 처리할 수 없다. (이를 처리하는것은 ROI가 떨어짐)

## 집계 윈도

### 텀블링 윈도

- 시간을 같은 크기의 겹치지 않는 구간으로 분할한다.
- 매 분 발생한 클릭 이벤트를 집계하기 적합하다.

### 슬라이딩 윈도

- 데이터 스트림을 미끄러져(sliding) 나아가면서 같은 시간 구간 안에 있는 이벤트를 집계한다.
- 슬라이딩 윈도는 서로 겹칠 수 있다.
- 지난 M분간 가장 많이 클릭된 상위 N개 광고를 알아내기에 적합하다.

## 전달 보장

- 시스템은 다음 질문에 답할 수 있어야 한다.
    - 이벤트의 중복 처리를 어떻게 피할 수 있는가?
    - 모든 이벤트의 처리를 어떻게 보장할 수 있는가?

### 어떤 전달 방식을 택할 것인가

- 약간의 중복은 괜찮다면 대체로 ‘최소 한 번’이 적절하다.
- 하지만 본 시스템은 ‘정확히 한 번’ 방식을 권장한다. (과금때문에..)

### 데이터 중복 제거

- 중복 데이터가 발생할 수 있는 상황
    - **한 클라이언트가 같은 이벤트를 여러 번 보낸다.**
        
        → 광고 사기/위험 제어 컴포넌트를 도입한다.
        
    - **집계 도중 집계 서비스 노드에서 장애가 발생하고, 업스트림 서비스가 ack를 받지 못했다.**
        
        → HDFS, S3 같은 외부 파일 저장소에 오프셋을 기록한다. (다운스트림에서 집계 결과 수신 확인 응답을 받은 후)
        
- 대규모 시스템에서 데이터 중복을 없애기는 쉽지 않다.

## 시스템 규모 확장

### MQ

- 생산자
    - 생산자 인스턴스 수에 제한을 두지 않으므로 확장성은 쉽게 달성할 수 있다.
- 소비자
    - 소비자 그룹 내의 재조저 매커니즘은 노드  추가/삭제를 통해 그 규모를 쉽게 조정할 수 있도록 한다.

### 브로커

- 해시 키
    - 같은 ad_id를 갖는 이벤트를 같은 카프카 파티션에 저장하기 위해 ad_id를 해시 키로 사용한다.
    - 집계 서비스는 같은 ad_id를 갖는 이벤트를 전부 같은 파티션에서 구독할 수 있다.
- 파티션의 수
    - 파티션의 수가 변하면 같은 ad_id를 갖는 이벤트가 다른 파티션에 기록될 수 있다.
    - 따라서 사전에 충분한 파티션을 확보하여 파티션 수가 동적으로 늘어나는 일은 피하는 것이 좋다.
- 토픽의 물리적 샤딩
    - 하나의 토픽만으로 충분한 경우는 거의 없다.
    - 데이터를 여러 토픽으로 나누면 시스템의 처리 대역폭을 높일 수 있지만, 복잡성이 증가하고 유지 관리 비용이 늘어난다.

### 집계 서비스

- 집계 서비스 규모는 노드의 추가/삭제를 통해 수평적으로 조정 가능하다.
- 집계 서비스의 처리 대역폭 높이기
    - ad_id마다 별도의 처리 스레드를 둔다.
    - 집계 서비스 노드를 아파치 하둡 YARN같은 자원 공급자에 배포한다. (다중 프로세싱)
        - 더 많은 컴퓨팅 자원을 추가하여 시스템 규모를 확장할 수 있다.

### 데이터베이스

- 카산드라는 **안정 해시**와 유사한 방식으로 수평적 규모 확장을 기본적으로 지원한다.
    - 데이터는 각 노드에 균등하게 분산한다.
    - 각 노드는 해시 링 위의 특정 해시 값 구간의 데이터 보관을 담당한다.
    - 클러스터에 새 노드를 추가하면 가상 노드 간 균형은 자동으로 다시 조정된다.

## 핫스팟 문제

- **핫스팟**: 다른 서비스나 샤드보다 더 많은 데이터를 수신하는 서비스나 샤드
- 더 많은 집계 서비스 노드를 할당하여 완화한다.
- 전역-지역 집계 또는 분할 고유 집계 방안을 활용한다.

## 결함 내성

- 집계는 메모리에서 이루어지므로 집계 노드에 장애가 생기면 집계 결과도 손실된다.
    
    → 업스트림 오프셋 같은 ‘시스템 상태’를 스냅숏으로 저장하고 마지막으로 저장된 상태부터 복구한다.
    
- 스냅숏을 이용하면 집계 서비스의 복구 절차가 단순해진다.
    - 장애 난 노드를 새 것으로 교체 후 마지막 스냅숏에서 데이터를 복구한다.
    - 스냅숏 이후에 도착한 새로운 이벤트는 새 집계 서비스 노드가 카프카 브로커에서 읽어가 다시 처리한다.

## 데이터 모니터링 및 정확성

### 지속적 모니터링

- **지연 시간**
    - 데이터 처리 각 단계마다 지연시간이 추가될 수 있으므로 시스템 중요 부분마다 timestamp 추적이 가능해야한다.
    - 기록된 시각 사이의 차이를 지연 시간 지표로 변환해서 모니터링한다.
- **메시지 큐 크기**
    - 큐의 크기가 갑자기 늘어난다면 더 많은 집계 서비스 노드를 추가해야 할 수 있다.
    - 카프카를 사용하는 경우 레코드 처리 지연 지표를 대신 추적하면 된다.

### 조정

- **조정**: 다양한 데이터를 비교하여 데이터 무결성을 보증하는 기법
- 매일 각 파티션에 기록된 클릭 이벤트를 이벤트 발생 시각에 따라 정렬한 결과를 일괄 처리하여 만들어 낸 다음, 실시간 집계 결과와 비교해본다.

## 대안적 설계안

- 광고 클릭 데이터를 하이브에 저장한 다음 빠른 질의는 ES 계층을 얹어서 처리한다.
- 집계는 클릭하우스나 드루이드같은 OLAP 데이터베이스를 통해 처리한다.

# 4단계. 마무리

- 데이터 모델 및 API 설계
- 맵리듀스 데이터 처리 패러다임을 통해 광고 클릭 이벤트를 집계하는 방안
- 메시지 큐, 집계 서비스, 데이터베이스의 규모 확장 방안
- 핫스팟 문제를 해결하는 방안
- 시스템의 지속적 모니터링
- 데이터 조정을 통한 정확성 보증 방안
- 결함 내성

<aside>
💡 워터마크 크기를 정할 때 기준 값으로 사용할만한 데이터는 어떤 것들이 있을까?

</aside>
